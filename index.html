<!DOCTYPE html>
<html lang="en" class="h-full bg-gray-100">

<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>The Ultimate AI Presenter</title>
  
  <script src="https://cdn.tailwindcss.com"></script>
  
  <script src="https://cdn.jsdelivr.net/npm/marked/marked.min.js"></script>

  <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@latest/dist/tf.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/@tensorflow-models/pose-detection@latest/dist/pose-detection.min.js"></script>

  <style>
    body {
      font-family: "Inter", -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, "Helvetica Neue", Arial, sans-serif;
      overflow: hidden;
    }
    
    /* Slide Content Styles (From File 1) */
    #slide-content h1 {
      font-size: 2.5rem; /* 40px */
      line-height: 1.2;
      font-weight: 700;
      margin-bottom: 1.5rem;
    }
    #slide-content h2 {
      font-size: 1.875rem; /* 30px */
      font-weight: 600;
      margin-bottom: 1.25rem;
    }
    #slide-content ul {
      list-style-type: disc;
      padding-left: 2.5rem;
      font-size: 1.5rem; /* 24px */
      line-height: 1.75;
    }
    #slide-content p {
      font-size: 1.5rem; /* 24px */
      line-height: 1.75;
    }

    /* Slide transition animation (From File 2) */
    .slide-anim {
      animation: fadeIn 0.5s ease-in-out;
    }
    @keyframes fadeIn {
      from { opacity: 0; }
      to { opacity: 1; }
    }
  </style>
</head>

<body class="h-full">

  <div id="input-view" class="flex h-full items-center justify-center bg-gray-900">
    <div class="w-full max-w-lg rounded-xl bg-white p-8 shadow-2xl">
      <h1 class="text-center text-3xl font-bold text-gray-900">Ultimate AI Presenter</h1>
      <p class="mt-2 text-center text-gray-600">What topic do you want to present on?</p>
      
      <div class="mt-6">
        <label for="topic-input" class="block text-sm font-medium text-gray-700">Presentation Topic</label>
        <textarea id="topic-input" rows="4" class="mt-1 block w-full rounded-md border-gray-300 p-3 shadow-sm focus:border-blue-500 focus:ring-blue-500" placeholder="e.g., 'A 5-slide presentation on the basics of neural networks'"></textarea>
      </div>
      
      <div class="mt-6">
        <button id="generate-btn" class="flex w-full justify-center rounded-md border border-transparent bg-blue-600 py-3 px-4 text-lg font-bold text-white shadow-sm hover:bg-blue-700 disabled:bg-gray-400">
          <span id="generate-btn-text">Generate & Present</span>
          <svg id="loading-spinner" class="animate-spin -ml-1 mr-3 h-5 w-5 text-white hidden" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24">
            <circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle>
            <path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path>
          </svg>
        </button>
      </div>
      
      <p id="status-message" class="mt-4 text-center text-sm text-red-600"></p>
    </div>
  </div>

  <div id="presentation-view" class="relative hidden h-full w-full flex-col items-center justify-center bg-gray-900 p-8">
    
    <div id="slide-container" class="flex w-full max-w-5xl aspect-[16/9] flex-col justify-center rounded-lg bg-white p-12 text-left shadow-xl text-gray-900">
      <div id="slide-content" class="w-full h-full">
        </div>
    </div>

    <div id="slide-counter" class="absolute bottom-20 text-lg font-medium text-gray-300 z-10">
      Slide 1 / 1
    </div>

    <video id="webcam" autoplay muted playsinline class="w-52 h-auto aspect-video bg-black fixed bottom-4 left-4 rounded-lg shadow-2xl border border-gray-700 z-20 object-cover"></video>

    <div class="fixed bottom-6 left-1/2 -translate-x-1/2 z-20 flex gap-4">
      <button id="prev-btn" class="px-5 py-2 bg-white bg-opacity-20 text-white rounded-lg shadow-lg backdrop-blur-sm hover:bg-opacity-30 transition-all active:scale-95 disabled:opacity-50">
        &larr; Prev
      </button>
      <button id="next-btn" class="px-5 py-2 bg-white bg-opacity-20 text-white rounded-lg shadow-lg backdrop-blur-sm hover:bg-opacity-30 transition-all active:scale-95 disabled:opacity-50">
        Next &rarr;
      </button>
    </div>
    
    <button id="back-btn" class="fixed top-6 left-6 rounded-lg bg-gray-600 py-2 px-4 text-sm font-medium text-white shadow-md hover:bg-gray-700 z-20">
      &larr; New Topic
    </button>
  </div>


  <script type="module">
    // --- STATE & CONSTANTS (MERGED) ---
    
    // Gemini API Config (From File 1)
    const MODEL_NAME = "gemini-2.5-flash-preview-09-2025";
    const apiKey = "AIzaSyCo1ps1aqgDtMUMBYD_9DraFyule8pTBZs"; // This is a placeholder
    const apiUrl = `https://generativelanguage.googleapis.com/v1beta/models/${MODEL_NAME}:generateContent?key=${apiKey}`;
    const systemPrompt = `You are a presentation-generation AI. The user will provide a topic.
You must generate a series of slides.
Respond *only* with a valid JSON array. Each object in the array should have a "title" (string) and "content" (string).
The "content" field should be formatted as simple Markdown (e.g., using <ul>, <li>, <p>, <strong>).
Do not include any other text, greetings, or backticks in your response. The response must be *only* the JSON array.
Example of your *only* output:
[
  {"title": "Slide 1 Title", "content": "<p>This is the content for the first slide.</p>"},
  {"title": "Slide 2 Title", "content": "<ul><li>Bullet point 1</li><li>Bullet point 2</li></ul>"}
]`;
    const jsonSchema = {
      "type": "ARRAY",
      "items": {
        "type": "OBJECT",
        "properties": {
          "title": { "type": "STRING" },
          "content": { "type": "STRING" }
        },
        "required": ["title", "content"]
      }
    };

    // App State
    let slides = [];
    let currentSlide = 0;
    let isLoading = false;
    
    // Gesture AI State (From File 2)
    let detector;
    let webcamStream;
    let isGestureCooldown = false;
    const GESTURE_COOLDOWN_MS = 1500; // 1.5 seconds

    // --- DOM ELEMENTS (MERGED) ---
    const inputView = document.getElementById("input-view");
    const presentationView = document.getElementById("presentation-view");
    
    const topicInput = document.getElementById("topic-input");
    const generateBtn = document.getElementById("generate-btn");
    const generateBtnText = document.getElementById("generate-btn-text");
    const loadingSpinner = document.getElementById("loading-spinner");
    const statusMessage = document.getElementById("status-message");
    
    const slideContainer = document.getElementById("slide-container");
    const slideContent = document.getElementById("slide-content");
    const slideCounter = document.getElementById("slide-counter");
    const prevBtn = document.getElementById("prev-btn");
    const nextBtn = document.getElementById("next-btn");
    const backBtn = document.getElementById("back-btn");
    
    const webcamVideo = document.getElementById('webcam');

    // --- VIEW FUNCTIONS (MERGED) ---
    
    function showInputView() {
      inputView.classList.remove("hidden");
      inputView.classList.add("flex");
      presentationView.classList.add("hidden");
      presentationView.classList.remove("flex");

      // Stop webcam and AI when returning to input
      stopWebcamAI();
    }
    
    function showPresentationView() {
      inputView.classList.add("hidden");
      inputView.classList.remove("flex");
      presentationView.classList.remove("hidden");
      presentationView.classList.add("flex");

      // Start the webcam and AI
      setupWebcamAI();
    }

    // --- PRESENTATION FUNCTIONS (MERGED) ---

    /**
     * This is the slide renderer from File 1.
     * It's perfect for rendering the text/HTML from Gemini.
     */
    function updateSlideContent() {
      if (slides.length === 0) return;
      
      const slide = slides[currentSlide];
      const contentHtml = marked.parse(slide.content);
      
      slideContent.innerHTML = `
        <h1 class="text-4xl font-bold mb-6">${slide.title}</h1>
        <div class="text-2xl leading-relaxed">${contentHtml}</div>
      `;
      
      // Add animation class
      slideContainer.classList.remove('slide-anim');
      void slideContainer.offsetWidth; // Force reflow
      slideContainer.classList.add('slide-anim');

      slideCounter.innerText = `Slide ${currentSlide + 1} / ${slides.length}`;
      updateButtonState();
    }

    function updateButtonState() {
      prevBtn.disabled = (currentSlide === 0);
      nextBtn.disabled = (currentSlide === slides.length - 1);
    }

    /**
     * These are the navigation functions from File 2.
     * They are superior because they include the gesture cooldown check.
     */
    function nextSlide() {
      if (currentSlide >= slides.length - 1 || isGestureCooldown) return;
      
      console.log("Moving to Next Slide");
      currentSlide++;
      updateSlideContent(); // Use File 1's render function
    }

    function prevSlide() {
      if (currentSlide <= 0 || isGestureCooldown) return;

      console.log("Moving to Previous Slide");
      currentSlide--;
      updateSlideContent(); // Use File 1's render function
    }

    // --- GEMINI API FUNCTIONS (From File 1) ---
    
    async function handleGenerate() {
      const userTopic = topicInput.value;
      if (!userTopic.trim()) {
        statusMessage.innerText = "Please enter a topic.";
        return;
      }
      
      setLoading(true);

      const payload = {
        systemInstruction: {
          parts: [{ "text": systemPrompt }]
        },
        contents: [{
          parts: [{ "text": userTopic }]
        }],
        generationConfig: {
          responseMimeType: "application/json",
          responseSchema: jsonSchema,
          temperature: 0.7,
        }
      };

      try {
        const result = await fetchWithRetry(apiUrl, {
          method: 'POST',
          headers: { 'Content-Type': 'application/json' },
          body: JSON.stringify(payload)
        });

        if (!result) {
          throw new Error("API response was empty.");
        }

        const jsonText = result.candidates?.[0]?.content?.parts?.[0]?.text;
        if (!jsonText) {
          throw new Error("Invalid response structure from API.");
        }

        slides = JSON.parse(jsonText);
        
        if (!Array.isArray(slides) || slides.length === 0) {
            throw new Error("AI did not return valid slide data.");
        }

        // Success! Reset state and show the presentation.
        currentSlide = 0;
        updateSlideContent();
        showPresentationView(); // This will now also start the webcam!

      } catch (error) {
        console.error("Error generating slides:", error);
        statusMessage.innerText = `Error: ${error.message}. Please try again.`;
      } finally {
        setLoading(false);
      }
    }
    
    function setLoading(isLoading) {
      if (isLoading) {
        generateBtn.disabled = true;
        generateBtnText.innerText = "Generating...";
        loadingSpinner.classList.remove("hidden");
        statusMessage.innerText = "";
      } else {
        generateBtn.disabled = false;
        generateBtnText.innerText = "Generate & Present";
        loadingSpinner.classList.add("hidden");
      }
    }

    async function fetchWithRetry(url, options, retries = 3, delay = 1000) {
      let lastError;
      for (let i = 0; i < retries; i++) {
        try {
          const response = await fetch(url, options);
          if (!response.ok) {
            throw new Error(`API Error: ${response.status} ${response.statusText}`);
          }
          return await response.json();
        } catch (error) {
          lastError = error;
          if (i < retries - 1) {
            await new Promise(resolve => setTimeout(resolve, delay * Math.pow(2, i)));
          }
        }
      }
      throw lastError;
    }

    // --- TENSORFLOW.JS AI FUNCTIONS (From File 2) ---

    async function setupWebcamAI() {
      // Don't re-setup if already running
      if (detector) return; 

      // 1. Get webcam feed
      try {
        webcamStream = await navigator.mediaDevices.getUserMedia({ video: true, audio: false });
        webcamVideo.srcObject = webcamStream;
      } catch (err) {
        console.error("Error accessing webcam:", err);
        webcamVideo.style.display = 'none';
        return; // Stop if no webcam
      }

      // 2. Wait for video to be ready
      await new Promise((resolve) => {
        webcamVideo.onloadedmetadata = () => {
          webcamVideo.play();
          resolve();
        };
      });

      // 3. Load the MoveNet detector
      try {
        const detectorConfig = {
          modelType: poseDetection.movenet.modelType.SINGLEPOSE_LIGHTNING
        };
        detector = await poseDetection.createDetector(poseDetection.SupportedModels.MoveNet, detectorConfig);
        console.log("MoveNet detector loaded.");
      } catch (err) {
        console.error("Error loading MoveNet model:", err);
        return; // Stop if model fails to load
      }

      // 4. Start the detection loop
      runDetectionLoop();
    }

    /**
     * Stops the webcam and clears the detector
     */
    function stopWebcamAI() {
      if (webcamStream) {
        webcamStream.getTracks().forEach(track => track.stop());
        webcamStream = null;
      }
      if (detector) {
        detector.dispose();
        detector = null;
        console.log("Webcam and AI stopped.");
      }
    }

    async function runDetectionLoop() {
      // Stop loop if detector was disposed (e.g., user went "Back")
      if (!detector) return; 

      try {
        const poses = await detector.estimatePoses(webcamVideo, {
          flipHorizontal: false
        });

        if (poses && poses.length > 0 && poses[0].keypoints) {
          handleGesture(poses[0].keypoints);
        }
      } catch (err) {
        console.error("Error during pose estimation:", err);
      }
      
      // Loop
      requestAnimationFrame(runDetectionLoop);
    }

    function handleGesture(keypoints) {
      if (isGestureCooldown) {
        return;
      }

      const getKeypoint = (name) => keypoints.find(kp => kp.name === name);

      const rightWrist = getKeypoint('right_wrist');
      const rightElbow = getKeypoint('right_elbow');
      const leftWrist = getKeypoint('left_wrist');
      const leftElbow = getKeypoint('left_elbow');
      const confidenceThreshold = 0.3;

      if (!rightWrist || !rightElbow || !leftWrist || !leftElbow ||
          rightWrist.score < confidenceThreshold ||
          rightElbow.score < confidenceThreshold ||
          leftWrist.score < confidenceThreshold ||
          leftElbow.score < confidenceThreshold) {
        return;
      }

      let gestureDetected = false;

      // "Next" gesture: Right wrist is above right elbow
      if (rightWrist.y < rightElbow.y) {
        console.log("Next Gesture (Right Hand Up) Detected!");
        nextSlide();
        gestureDetected = true;
      } 
      // "Prev" gesture: Left wrist is above left elbow
      else if (leftWrist.y < leftElbow.y) {
        console.log("Prev Gesture (Left Hand Up) Detected!");
        prevSlide();
        gestureDetected = true;
      }

      if (gestureDetected) {
        isGestureCooldown = true;
        webcamVideo.style.borderColor = 'rgb(59 130 246)'; // Blue
        webcamVideo.style.borderWidth = '4px';

        setTimeout(() => {
          isGestureCooldown = false;
          webcamVideo.style.borderColor = 'rgb(55 65 81)'; // Gray
          webcamVideo.style.borderWidth = '1px';
        }, GESTURE_COOLDOWN_MS);
      }
    }


    // --- EVENT LISTENERS (MERGED) ---
    generateBtn.addEventListener("click", handleGenerate);
    prevBtn.addEventListener("click", prevSlide);
    nextBtn.addEventListener("click", nextSlide);
    backBtn.addEventListener("click", showInputView);

    // --- INITIALIZATION ---
    showInputView(); // Start on the input screen
    
  </script>

</body>
</html>